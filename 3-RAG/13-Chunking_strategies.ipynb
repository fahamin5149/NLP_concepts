{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ZMJRBtXS-e"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWmKrrc2VILP",
        "outputId": "90fdcb17-f3a2-40ba-a877-2f05f9678358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.0/397.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-core langchain_google_genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgDSAaKVXX5p"
      },
      "source": [
        "# Langchain and Groq api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HRDUbq0rWdw8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "langchain_api= userdata.get('langchain_api')\n",
        "groq_api= userdata.get('groq_api')\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api\n",
        "os.environ['GROQ_API_KEY'] = groq_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rsn7_vv1jy5",
        "outputId": "b441f9c3-6f58-4e10-b944-4fd0cd5d56ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "W2KaKPvh1qsw",
        "outputId": "c7b517a3-314e-41e9-dc90-1f7689bd1038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(docs))\n",
        "docs[0].page_content[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character text splitting"
      ],
      "metadata": {
        "id": "U4VXNnhInj9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Character text splitting\n",
        "text= \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "\"\"\"\n",
        "\n",
        "## manual text splitting\n",
        "chunks=[]\n",
        "chunk_size=35\n",
        "# text=docs[0].page_content\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i+chunk_size]\n",
        "    chunks.append(chunk)\n",
        "\n",
        "documents= [Document(page_content=chunk, meteadata= {\"souce\":\"local\"}) for chunk in chunks]\n",
        "print(documents)\n",
        "\n",
        "## Automatic text splitting\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0)\n",
        "chunks = text_splitter.create_documents(documents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0gH7HqjnooK",
        "outputId": "ef2d0bfd-0f32-461e-9808-144f757cbbea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='You are an AI language model assist'), Document(page_content='ant. Your task is to generate five\\n'), Document(page_content='different versions of the given use'), Document(page_content='r question to retrieve relevant doc'), Document(page_content='uments from a vector\\ndatabase. By g'), Document(page_content='enerating multiple perspectives on '), Document(page_content='the user question, your goal is to '), Document(page_content='help\\nthe user overcome some of the '), Document(page_content='limitations of the distance-based s'), Document(page_content='imilarity search.\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive text splitter"
      ],
      "metadata": {
        "id": "HVc-69dLsbjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter= RecursiveCharacterTextSplitter(chunk_size= 150, chunk_overlap= 0)\n",
        "chunks= text_splitter.create_documents([text])\n",
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSP_y_0DqjJM",
        "outputId": "87c1b12a-3466-4947-d233-a9108fbe0065"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='You are an AI language model assistant. Your task is to generate five'),\n",
              " Document(page_content='different versions of the given user question to retrieve relevant documents from a vector'),\n",
              " Document(page_content='database. By generating multiple perspectives on the user question, your goal is to help'),\n",
              " Document(page_content='the user overcome some of the limitations of the distance-based similarity search.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markdown text splitter"
      ],
      "metadata": {
        "id": "rwSzt1RGxljz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Document Specific Splitting\n",
        "print(\"#### Document Specific Splitting ####\")\n",
        "\n",
        "# Document Specific Splitting - Markdown\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
        "markdown_text = \"\"\"\n",
        "# Fun in California\n",
        "\n",
        "## Driving\n",
        "\n",
        "Try driving on the 1 down to San Diego\n",
        "\n",
        "### Food\n",
        "\n",
        "Make sure to eat a burrito while you're there\n",
        "\n",
        "## Hiking\n",
        "\n",
        "Go to Yosemite\n",
        "\"\"\"\n",
        "print(splitter.create_documents([markdown_text]))"
      ],
      "metadata": {
        "id": "lDMGGEV6xoRA",
        "outputId": "96697a92-c92c-4840-ae43-bfbdc173d48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Document Specific Splitting ####\n",
            "[Document(page_content='# Fun in California\\n\\n## Driving'), Document(page_content='Try driving on the 1 down to San Diego'), Document(page_content='### Food'), Document(page_content=\"Make sure to eat a burrito while you're\"), Document(page_content='there'), Document(page_content='## Hiking\\n\\nGo to Yosemite')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python text splitter"
      ],
      "metadata": {
        "id": "GSeIxE2exzQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Specific Splitting - Python\n",
        "from langchain.text_splitter import PythonCodeTextSplitter\n",
        "python_text = \"\"\"\n",
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "\n",
        "for i in range(10):\n",
        "    print (i)\n",
        "\"\"\"\n",
        "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "print(python_splitter.create_documents([python_text]))"
      ],
      "metadata": {
        "id": "E1-oumC7x1qI",
        "outputId": "53d08960-08bb-4279-810f-9b3b950c10cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'), Document(page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Javascript splitting"
      ],
      "metadata": {
        "id": "wiayL3S2wycO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Specific Splitting - Javascript\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "javascript_text = \"\"\"\n",
        "// Function is called, the return value will end up in x\n",
        "let x = myFunction(4, 3);\n",
        "\n",
        "function myFunction(a, b) {\n",
        "// Function returns the product of a and b\n",
        "  return a * b;\n",
        "}\n",
        "\"\"\"\n",
        "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
        ")\n",
        "print(js_splitter.create_documents([javascript_text]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k56tYbLYqwp2",
        "outputId": "a8d4e34e-f521-45e9-8f12-5ea7637e83fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='// Function is called, the return value will end up in x'), Document(page_content='let x = myFunction(4, 3);'), Document(page_content='function myFunction(a, b) {'), Document(page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM"
      ],
      "metadata": {
        "id": "MfIHN8We1ePz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLweVz7wfWtS",
        "outputId": "acc6e37a-416b-4089-c542-b4b5e673897a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.10/dist-packages (0.0.65)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement agentic_chunker (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for agentic_chunker\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain_groq sentence-transformers langchain_experimental agentic_chunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HGrTXjLNsXKx"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "# creating multi queries\n",
        "LLM = ChatGroq(model_name='llama-3.1-70b-versatile')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Chunking"
      ],
      "metadata": {
        "id": "yjkCEAAlw9qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "# Initialize the Hugging Face Sentence Transformer model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "_fHNBVqC67KW",
        "outputId": "e0537b41-54ab-4ff9-befc-4538050b3b4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "text_splitter= SemanticChunker(embeddings)\n",
        "SemanticChunker(\n",
        "    embeddings, breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
        ")\n",
        "documents = text_splitter.create_documents([text])\n",
        "(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QclTOGV1w2El",
        "outputId": "905b2ded-49f9-4ce7-88c5-30e5dc0958de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='You are an AI language model assistant. Your task is to generate five\\ndifferent versions of the given user question to retrieve relevant documents from a vector\\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search.'),\n",
              " Document(page_content='')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic Chunking\n",
        "### Proposition based chunking"
      ],
      "metadata": {
        "id": "GAfGQP3PyieJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/pdf/2312.06648.pdf\n",
        "\n",
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from typing import Optional, List\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain import hub\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "# creating multi queries\n",
        "llm = ChatGroq(model_name='llama-3.1-70b-versatile')\n",
        "\n",
        "# https://smith.langchain.com/hub/wfh/proposal-indexing?organizationId=65e2223e-316a-5256-b012-5033801a97fa\n",
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "\n",
        "class Sentences(BaseModel):\n",
        "    sentences: List[str]\n",
        "\n",
        "text= docs[0].page_content\n",
        "runnable = obj | llm\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n",
        "\n",
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({\n",
        "    \t\"input\": text\n",
        "    }).content\n",
        "    propositions = extraction_chain.invoke(runnable_output)[\"text\"][0].sentences\n",
        "    return propositions\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "text_propositions = []\n",
        "for i, para in enumerate(paragraphs[:5]):\n",
        "    propositions = get_propositions(para)\n",
        "    text_propositions.extend(propositions)\n",
        "    print (f\"Done with {i}\")\n"
      ],
      "metadata": {
        "id": "Wy30dAHhyO9a",
        "outputId": "060ad55b-e402-4524-ab2d-4a1f9b58d698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:5301: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  prompt = loads(json.dumps(prompt_object.manifest))\n",
            "<ipython-input-24-06080af1278a>:27: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
            "  extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with 0\n",
            "Done with 1\n",
            "Done with 2\n",
            "Done with 3\n",
            "Done with 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You have {len(text_propositions)} propositions\")\n",
        "(text_propositions[:10])\n"
      ],
      "metadata": {
        "id": "JRA3yXYR2jHs",
        "outputId": "a094cd35-dbb1-41c5-cbcf-c2d25a252807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 18 propositions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"It seems like you haven't provided the text to be decomposed.\",\n",
              " \"Please provide the text, and I'll be happy to help you decompose it into clear and simple propositions, following the rules you provided earlier.\",\n",
              " \"The article 'LLM Powered Autonomous Agents' was written on June 23, 2023.\",\n",
              " 'Building agents with a large language model (LLM) as its core controller is a cool concept.',\n",
              " 'Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.',\n",
              " 'AutoGPT is a proof-of-concept demo.',\n",
              " 'GPT-Engineer is a proof-of-concept demo.',\n",
              " 'BabyAGI is a proof-of-concept demo.',\n",
              " 'The potentiality of a large language model (LLM) extends beyond generating well-written copies.',\n",
              " 'The potentiality of a large language model (LLM) extends beyond generating stories.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping the meaningful chunks together"
      ],
      "metadata": {
        "id": "S7uQcI8J3QXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(chunks, collection_name):\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        collection_name=collection_name,\n",
        "        embedding=embeddings,\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    prompt_template = \"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | LLM\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    result = chain.invoke(\"What is the use of Text Splitting?\")\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "VaNpmMIW557r"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac = AgenticChunker()\n",
        "ac.add_propositions(text_propositions)\n",
        "print(ac.pretty_print_chunks())\n",
        "chunks = ac.get_chunks(get_type='list_of_strings')\n",
        "print(chunks)\n",
        "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
        "rag(documents, \"agentic-chunks\")"
      ],
      "metadata": {
        "id": "XVYGA7BN2pZy",
        "outputId": "117fc1a1-2c72-46b9-9900-fd418751e376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'It seems like you haven'\u001b[0mt provided the text to be decomposed.'\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'It seems like you haven'</span>t provided the text to be decomposed.'\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks, creating a new one\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks, creating a new one\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0m1b1d5\u001b[1m)\u001b[0m: Overview\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>1b1d5<span style=\"font-weight: bold\">)</span>: Overview\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'Please provide the text, and I'\u001b[0mll be happy to help you decompose it into clear and simple propositions, \n",
              "following the rules you provided earlier.'\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Please provide the text, and I'</span>ll be happy to help you decompose it into clear and simple propositions, \n",
              "following the rules you provided earlier.'\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m1b1d5\u001b[1m)\u001b[0m, adding to: Overview\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>1b1d5<span style=\"font-weight: bold\">)</span>, adding to: Overview\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The article '\u001b[0mLLM Powered Autonomous Agents' was written on June \u001b[1;36m23\u001b[0m, \u001b[1;36m2023\u001b[0m.'\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The article '</span>LLM Powered Autonomous Agents' was written on June <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>.'\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m1b1d5\u001b[1m)\u001b[0m, adding to: General Information\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>1b1d5<span style=\"font-weight: bold\">)</span>, adding to: General Information\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'Building agents with a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as its core controller is a cool concept.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Building agents with a large language model (LLM) as its core controller is a cool concept.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m1b1d5\u001b[1m)\u001b[0m, adding to: Date & Times\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>1b1d5<span style=\"font-weight: bold\">)</span>, adding to: Date &amp; Times\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks found\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks found\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m: AI & Machine Learning\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>: AI &amp; Machine Learning\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'AutoGPT is a proof-of-concept demo.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'AutoGPT is a proof-of-concept demo.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: AI & Machine Learning\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: AI &amp; Machine Learning\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'GPT-Engineer is a proof-of-concept demo.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'GPT-Engineer is a proof-of-concept demo.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Tech\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Tech\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'BabyAGI is a proof-of-concept demo.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'BabyAGI is a proof-of-concept demo.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Machine Learning\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Machine Learning\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating well-written copies.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The potentiality of a large language model (LLM) extends beyond generating well-written copies.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Machine Learning\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Machine Learning\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating stories.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The potentiality of a large language model (LLM) extends beyond generating stories.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating essays.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The potentiality of a large language model (LLM) extends beyond generating essays.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Machine Learning\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Machine Learning\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating programs.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The potentiality of a large language model (LLM) extends beyond generating programs.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Language\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Language\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'A large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can be framed as a powerful general problem solver.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'A large language model (LLM) can be framed as a powerful general problem solver.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence, Language, & Automation\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence, Language, &amp; Automation\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks found\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks found\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0m9326d\u001b[1m)\u001b[0m: Large Language Models\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>9326d<span style=\"font-weight: bold\">)</span>: Large Language Models\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'A LLM-powered autonomous agent system includes several key components in addition to the LLM.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'A LLM-powered autonomous agent system includes several key components in addition to the LLM.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m9326d\u001b[1m)\u001b[0m, adding to: Large Language Models\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>9326d<span style=\"font-weight: bold\">)</span>, adding to: Large Language Models\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'Since the provided content is just a single word, '\u001b[0mPlanning', I will not be able to perform the \n",
              "decomposition process as it does not contain any compound sentences, named entities, or descriptive information.'\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Since the provided content is just a single word, '</span>Planning', I will not be able to perform the \n",
              "decomposition process as it does not contain any compound sentences, named entities, or descriptive information.'\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m1b1d5\u001b[1m)\u001b[0m, adding to: Articles & Publications\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>1b1d5<span style=\"font-weight: bold\">)</span>, adding to: Articles &amp; Publications\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'However, I can still provide an empty list as per the JSON format, indicating that there are no \u001b[0m\n",
              "\u001b[32mpropositions to decompose.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'However, I can still provide an empty list as per the JSON format, indicating that there are no </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">propositions to decompose.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m567f3\u001b[1m)\u001b[0m, adding to: Artificial Intelligence & Automation\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>567f3<span style=\"font-weight: bold\">)</span>, adding to: Artificial Intelligence &amp; Automation\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The agent breaks down large tasks into smaller, manageable subgoals.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The agent breaks down large tasks into smaller, manageable subgoals.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "You have \u001b[1;36m3\u001b[0m chunks\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "You have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> chunks\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: 1b1d5\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: 1b1d5\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about articles, including their content, publication dates, and related \n",
              "concepts.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about articles, including their content, publication dates, and related \n",
              "concepts.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -It seems like you haven't provided the text to be decomposed.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -It seems like you haven't provided the text to be decomposed.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -Please provide the text, and I'll be happy to help you decompose it into clear and simple propositions, \n",
              "following the rules you provided earlier.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Please provide the text, and I'll be happy to help you decompose it into clear and simple propositions, \n",
              "following the rules you provided earlier.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The article \u001b[32m'LLM Powered Autonomous Agents'\u001b[0m was written on June \u001b[1;36m23\u001b[0m, \u001b[1;36m2023\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The article <span style=\"color: #008000; text-decoration-color: #008000\">'LLM Powered Autonomous Agents'</span> was written on June <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -Building agents with a large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m as its core controller is a cool concept.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Building agents with a large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> as its core controller is a cool concept.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -Since the provided content is just a single word, \u001b[32m'Planning'\u001b[0m, I will not be able to perform the decomposition \n",
              "process as it does not contain any compound sentences, named entities, or descriptive information.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Since the provided content is just a single word, <span style=\"color: #008000; text-decoration-color: #008000\">'Planning'</span>, I will not be able to perform the decomposition \n",
              "process as it does not contain any compound sentences, named entities, or descriptive information.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: 567f3\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: 567f3\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about the capabilities and applications of artificial intelligence, \n",
              "including examples and potential uses of language models in various tasks, such as writing, problem-solving, and \n",
              "more.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about the capabilities and applications of artificial intelligence, \n",
              "including examples and potential uses of language models in various tasks, such as writing, problem-solving, and \n",
              "more.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -AutoGPT is a proof-of-concept demo.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -AutoGPT is a proof-of-concept demo.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -GPT-Engineer is a proof-of-concept demo.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -GPT-Engineer is a proof-of-concept demo.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -BabyAGI is a proof-of-concept demo.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -BabyAGI is a proof-of-concept demo.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The potentiality of a large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m extends beyond generating well-written copies.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The potentiality of a large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> extends beyond generating well-written copies.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The potentiality of a large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m extends beyond generating stories.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The potentiality of a large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> extends beyond generating stories.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The potentiality of a large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m extends beyond generating essays.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The potentiality of a large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> extends beyond generating essays.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The potentiality of a large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m extends beyond generating programs.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The potentiality of a large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> extends beyond generating programs.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -A large language model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m can be framed as a powerful general problem solver.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -A large language model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> can be framed as a powerful general problem solver.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -However, I can still provide an empty list as per the JSON format, indicating that there are no propositions \n",
              "to decompose.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -However, I can still provide an empty list as per the JSON format, indicating that there are no propositions \n",
              "to decompose.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: 9326d\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: 9326d\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about the architecture, components, and functions of artificial \n",
              "intelligence systems, including those powered by large language models.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about the architecture, components, and functions of artificial \n",
              "intelligence systems, including those powered by large language models.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -A LLM-powered autonomous agent system includes several key components in addition to the LLM.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -A LLM-powered autonomous agent system includes several key components in addition to the LLM.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The agent breaks down large tasks into smaller, manageable subgoals.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The agent breaks down large tasks into smaller, manageable subgoals.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;35mNone\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m\"It seems like you haven't provided the text to be decomposed. Please provide the text, and I'll be happy to \u001b[0m\n",
              "\u001b[32mhelp you decompose it into clear and simple propositions, following the rules you provided earlier. The article \u001b[0m\n",
              "\u001b[32m'LLM Powered Autonomous Agents' was written on June 23, 2023. Building agents with a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as \u001b[0m\n",
              "\u001b[32mits core controller is a cool concept. Since the provided content is just a single word, 'Planning', I will not be \u001b[0m\n",
              "\u001b[32mable to perform the decomposition process as it does not contain any compound sentences, named entities, or \u001b[0m\n",
              "\u001b[32mdescriptive information.\"\u001b[0m,\n",
              "    \u001b[32m'Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples. \u001b[0m\n",
              "\u001b[32mAutoGPT is a proof-of-concept demo. GPT-Engineer is a proof-of-concept demo. BabyAGI is a proof-of-concept demo. \u001b[0m\n",
              "\u001b[32mThe potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating well-written copies. The potentiality of\u001b[0m\n",
              "\u001b[32ma large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating stories. The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mextends beyond generating essays. The potentiality of a large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m extends beyond generating \u001b[0m\n",
              "\u001b[32mprograms. A large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can be framed as a powerful general problem solver. However, I can still \u001b[0m\n",
              "\u001b[32mprovide an empty list as per the JSON format, indicating that there are no propositions to decompose.'\u001b[0m,\n",
              "    \u001b[32m'In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain. A LLM-powered autonomous \u001b[0m\n",
              "\u001b[32magent system includes several key components in addition to the LLM. The agent breaks down large tasks into \u001b[0m\n",
              "\u001b[32msmaller, manageable subgoals.'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"It seems like you haven't provided the text to be decomposed. Please provide the text, and I'll be happy to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">help you decompose it into clear and simple propositions, following the rules you provided earlier. The article </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'LLM Powered Autonomous Agents' was written on June 23, 2023. Building agents with a large language model (LLM) as </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">its core controller is a cool concept. Since the provided content is just a single word, 'Planning', I will not be </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">able to perform the decomposition process as it does not contain any compound sentences, named entities, or </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">descriptive information.\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">AutoGPT is a proof-of-concept demo. GPT-Engineer is a proof-of-concept demo. BabyAGI is a proof-of-concept demo. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">The potentiality of a large language model (LLM) extends beyond generating well-written copies. The potentiality of</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a large language model (LLM) extends beyond generating stories. The potentiality of a large language model (LLM) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">extends beyond generating essays. The potentiality of a large language model (LLM) extends beyond generating </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">programs. A large language model (LLM) can be framed as a powerful general problem solver. However, I can still </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">provide an empty list as per the JSON format, indicating that there are no propositions to decompose.'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'In a LLM-powered autonomous agent system, the LLM functions as the agent’s brain. A LLM-powered autonomous </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">agent system includes several key components in addition to the LLM. The agent breaks down large tasks into </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">smaller, manageable subgoals.'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The provided context does not explicitly mention the use of \u001b[32m\"Text Splitting.\"\u001b[0m However, it does mention breaking \n",
              "down large tasks into smaller, manageable subgoals in the context of a LLM-powered autonomous agent system.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The provided context does not explicitly mention the use of <span style=\"color: #008000; text-decoration-color: #008000\">\"Text Splitting.\"</span> However, it does mention breaking \n",
              "down large tasks into smaller, manageable subgoals in the context of a LLM-powered autonomous agent system.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import uuid\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from typing import Optional\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from dotenv import load_dotenv\n",
        "from rich import print\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class AgenticChunker:\n",
        "    def __init__(self, openai_api_key=None):\n",
        "        self.chunks = {}\n",
        "        self.id_truncate_limit = 5\n",
        "\n",
        "        # Whether or not to update/refine summaries and titles as you get new information\n",
        "        self.generate_new_metadata_ind = True\n",
        "        self.print_logging = True\n",
        "\n",
        "        self.llm = LLM\n",
        "\n",
        "    def add_propositions(self, propositions):\n",
        "        for proposition in propositions:\n",
        "            self.add_proposition(proposition)\n",
        "\n",
        "    def add_proposition(self, proposition):\n",
        "        if self.print_logging:\n",
        "            print (f\"\\nAdding: '{proposition}'\")\n",
        "\n",
        "        # If it's your first chunk, just make a new chunk and don't check for others\n",
        "        if len(self.chunks) == 0:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks, creating a new one\")\n",
        "            self._create_new_chunk(proposition)\n",
        "            return\n",
        "\n",
        "        chunk_id = self._find_relevant_chunk(proposition)\n",
        "\n",
        "        # If a chunk was found then add the proposition to it\n",
        "        if chunk_id:\n",
        "            if self.print_logging:\n",
        "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
        "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
        "            return\n",
        "        else:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks found\")\n",
        "            # If a chunk wasn't found, then create a new one\n",
        "            self._create_new_chunk(proposition)\n",
        "\n",
        "\n",
        "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
        "        # Add then\n",
        "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
        "\n",
        "        # Then grab a new summary\n",
        "        if self.generate_new_metadata_ind:\n",
        "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
        "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
        "\n",
        "    def _update_chunk_summary(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the chunk new summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary']\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _update_chunk_title(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good title will say what the chunk is about.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
        "\n",
        "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        updated_chunk_title = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary'],\n",
        "            \"current_title\" : chunk['title']\n",
        "        }).content\n",
        "\n",
        "        return updated_chunk_title\n",
        "\n",
        "    def _get_new_chunk_summary(self, proposition):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the new chunk summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": proposition\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _get_new_chunk_title(self, summary):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good chunk title is brief but encompasses what the chunk is about\n",
        "\n",
        "                    You will be given a summary of a chunk which needs a title\n",
        "\n",
        "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_title = runnable.invoke({\n",
        "            \"summary\": summary\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_title\n",
        "\n",
        "\n",
        "    def _create_new_chunk(self, proposition):\n",
        "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
        "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
        "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
        "\n",
        "        self.chunks[new_chunk_id] = {\n",
        "            'chunk_id' : new_chunk_id,\n",
        "            'propositions': [proposition],\n",
        "            'title' : new_chunk_title,\n",
        "            'summary': new_chunk_summary,\n",
        "            'chunk_index' : len(self.chunks)\n",
        "        }\n",
        "        if self.print_logging:\n",
        "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
        "\n",
        "    def get_chunk_outline(self):\n",
        "        \"\"\"\n",
        "        Get a string which represents the chunks you currently have.\n",
        "        This will be empty when you first start off\n",
        "        \"\"\"\n",
        "        chunk_outline = \"\"\n",
        "\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            single_chunk_string = f\"\"\"Chunk ({chunk['chunk_id']}): {chunk['title']}\\nSummary: {chunk['summary']}\\n\\n\"\"\"\n",
        "\n",
        "            chunk_outline += single_chunk_string\n",
        "\n",
        "        return chunk_outline\n",
        "\n",
        "    def _find_relevant_chunk(self, proposition):\n",
        "        current_chunk_outline = self.get_chunk_outline()\n",
        "\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
        "\n",
        "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
        "                    The goal is to group similar propositions and chunks.\n",
        "\n",
        "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
        "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
        "\n",
        "                    Example:\n",
        "                    Input:\n",
        "                        - Proposition: \"Greg really likes hamburgers\"\n",
        "                        - Current Chunks:\n",
        "                            - Chunk ID: 2n4l3d\n",
        "                            - Chunk Name: Places in San Francisco\n",
        "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
        "\n",
        "                            - Chunk ID: 93833k\n",
        "                            - Chunk Name: Food Greg likes\n",
        "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
        "                    Output: 93833k\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
        "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        chunk_found = runnable.invoke({\n",
        "            \"proposition\": proposition,\n",
        "            \"current_chunk_outline\": current_chunk_outline\n",
        "        }).content\n",
        "\n",
        "        # Pydantic data class\n",
        "        class ChunkID(BaseModel):\n",
        "            \"\"\"Extracting the chunk id\"\"\"\n",
        "            chunk_id: Optional[str]\n",
        "\n",
        "        # Extraction to catch-all LLM responses. This is a bandaid\n",
        "        extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n",
        "        extraction_found = extraction_chain.invoke(chunk_found)[\"text\"]\n",
        "        if extraction_found:\n",
        "            chunk_found = extraction_found[0].chunk_id\n",
        "\n",
        "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
        "        # So return nothing\n",
        "        if len(chunk_found) != self.id_truncate_limit:\n",
        "            return None\n",
        "\n",
        "        return chunk_found\n",
        "\n",
        "    def get_chunks(self, get_type='dict'):\n",
        "        \"\"\"\n",
        "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
        "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
        "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
        "        \"\"\"\n",
        "        if get_type == 'dict':\n",
        "            return self.chunks\n",
        "        if get_type == 'list_of_strings':\n",
        "            chunks = []\n",
        "            for chunk_id, chunk in self.chunks.items():\n",
        "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
        "            return chunks\n",
        "\n",
        "    def pretty_print_chunks(self):\n",
        "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
        "            print(f\"Chunk ID: {chunk_id}\")\n",
        "            print(f\"Summary: {chunk['summary']}\")\n",
        "            print(f\"Propositions:\")\n",
        "            for prop in chunk['propositions']:\n",
        "                print(f\"    -{prop}\")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "    def pretty_print_chunk_outline(self):\n",
        "        print (\"Chunk Outline\\n\")\n",
        "        print(self.get_chunk_outline())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ac = AgenticChunker()\n",
        "\n",
        "    ## Comment and uncomment the propositions to your hearts content\n",
        "    propositions = [\n",
        "        'The month is October.',\n",
        "        'The year is 2023.',\n",
        "        \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\",\n",
        "        'Teachers and coaches implicitly told us that the returns were linear.',\n",
        "        \"I heard a thousand times that 'You get out what you put in.'\",\n",
        "        # 'Teachers and coaches meant well.',\n",
        "        # \"The statement that 'You get out what you put in' is rarely true.\",\n",
        "        # \"If your product is only half as good as your competitor's product, you do not get half as many customers.\",\n",
        "        # \"You get no customers if your product is only half as good as your competitor's product.\",\n",
        "        # 'You go out of business if you get no customers.',\n",
        "        # 'The returns for performance are superlinear in business.',\n",
        "        # 'Some people think the superlinear returns for performance are a flaw of capitalism.',\n",
        "        # 'Some people think that changing the rules of capitalism would stop the superlinear returns for performance from being true.',\n",
        "        # 'Superlinear returns for performance are a feature of the world.',\n",
        "        # 'Superlinear returns for performance are not an artifact of rules that humans have invented.',\n",
        "        # 'The same pattern of superlinear returns is observed in fame.',\n",
        "        # 'The same pattern of superlinear returns is observed in power.',\n",
        "        # 'The same pattern of superlinear returns is observed in military victories.',\n",
        "        # 'The same pattern of superlinear returns is observed in knowledge.',\n",
        "        # 'The same pattern of superlinear returns is observed in benefit to humanity.',\n",
        "        # 'In fame, power, military victories, knowledge, and benefit to humanity, the rich get richer.'\n",
        "    ]\n",
        "\n",
        "    ac.add_propositions(propositions)\n",
        "    ac.pretty_print_chunks()\n",
        "    ac.pretty_print_chunk_outline()\n",
        "    print (ac.get_chunks(get_type='list_of_strings'))"
      ],
      "metadata": {
        "id": "misD84Qx4XwZ",
        "outputId": "125753ec-7bca-45bf-9a57-c8be288427d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The month is October.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The month is October.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks, creating a new one\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks, creating a new one\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0m68b8d\u001b[1m)\u001b[0m: Date & Time\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>68b8d<span style=\"font-weight: bold\">)</span>: Date &amp; Time\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'The year is 2023.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'The year is 2023.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0m68b8d\u001b[1m)\u001b[0m, adding to: Date & Time\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>68b8d<span style=\"font-weight: bold\">)</span>, adding to: Date &amp; Time\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'One of the most important things that I didn'\u001b[0mt understand about the world as a child was the degree to \n",
              "which the returns for performance are superlinear.'\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'One of the most important things that I didn'</span>t understand about the world as a child was the degree to \n",
              "which the returns for performance are superlinear.'\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks found\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks found\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0m\u001b[1;36m92230\u001b[0m\u001b[1m)\u001b[0m: Performance and Rewards\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92230</span><span style=\"font-weight: bold\">)</span>: Performance and Rewards\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'Teachers and coaches implicitly told us that the returns were linear.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'Teachers and coaches implicitly told us that the returns were linear.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No chunks found\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No chunks found\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Created new chunk \u001b[1m(\u001b[0me4cee\u001b[1m)\u001b[0m: Life Guidance\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Created new chunk <span style=\"font-weight: bold\">(</span>e4cee<span style=\"font-weight: bold\">)</span>: Life Guidance\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Adding: \u001b[32m'I heard a thousand times that '\u001b[0mYou get out what you put in.\u001b[32m''\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Adding: <span style=\"color: #008000; text-decoration-color: #008000\">'I heard a thousand times that '</span>You get out what you put in.<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Found \u001b[1m(\u001b[0me4cee\u001b[1m)\u001b[0m, adding to: Life Guidance\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Found <span style=\"font-weight: bold\">(</span>e4cee<span style=\"font-weight: bold\">)</span>, adding to: Life Guidance\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "You have \u001b[1;36m3\u001b[0m chunks\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "You have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> chunks\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: 68b8d\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: 68b8d\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about specific dates and times.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about specific dates and times.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The month is October.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The month is October.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -The year is \u001b[1;36m2023\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -The year is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: \u001b[1;36m92230\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92230</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about the relationship between performance and returns or rewards.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about the relationship between performance and returns or rewards.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -One of the most important things that I didn't understand about the world as a child was the degree to which \n",
              "the returns for performance are superlinear.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -One of the most important things that I didn't understand about the world as a child was the degree to which \n",
              "the returns for performance are superlinear.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk #\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk #<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk ID: e4cee\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk ID: e4cee\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary: This chunk contains information about advice and lessons given by authority figures or mentors, \n",
              "particularly regarding effort and reward.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: This chunk contains information about advice and lessons given by authority figures or mentors, \n",
              "particularly regarding effort and reward.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Propositions:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Propositions:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -Teachers and coaches implicitly told us that the returns were linear.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -Teachers and coaches implicitly told us that the returns were linear.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    -I heard a thousand times that \u001b[32m'You get out what you put in.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    -I heard a thousand times that <span style=\"color: #008000; text-decoration-color: #008000\">'You get out what you put in.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk Outline\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk Outline\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunk \u001b[1m(\u001b[0m68b8d\u001b[1m)\u001b[0m: Date & Times\n",
              "Summary: This chunk contains information about specific dates and times.\n",
              "\n",
              "Chunk \u001b[1m(\u001b[0m\u001b[1;36m92230\u001b[0m\u001b[1m)\u001b[0m: Performance and Rewards\n",
              "Summary: This chunk contains information about the relationship between performance and returns or rewards.\n",
              "\n",
              "Chunk \u001b[1m(\u001b[0me4cee\u001b[1m)\u001b[0m: Motivation & Advice\n",
              "Summary: This chunk contains information about advice and lessons given by authority figures or mentors, \n",
              "particularly regarding effort and reward.\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chunk <span style=\"font-weight: bold\">(</span>68b8d<span style=\"font-weight: bold\">)</span>: Date &amp; Times\n",
              "Summary: This chunk contains information about specific dates and times.\n",
              "\n",
              "Chunk <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92230</span><span style=\"font-weight: bold\">)</span>: Performance and Rewards\n",
              "Summary: This chunk contains information about the relationship between performance and returns or rewards.\n",
              "\n",
              "Chunk <span style=\"font-weight: bold\">(</span>e4cee<span style=\"font-weight: bold\">)</span>: Motivation &amp; Advice\n",
              "Summary: This chunk contains information about advice and lessons given by authority figures or mentors, \n",
              "particularly regarding effort and reward.\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m'The month is October. The year is 2023.'\u001b[0m,\n",
              "    \u001b[32m\"One of the most important things that I didn't understand about the world as a child was the degree to which \u001b[0m\n",
              "\u001b[32mthe returns for performance are superlinear.\"\u001b[0m,\n",
              "    \u001b[32m\"Teachers and coaches implicitly told us that the returns were linear. I heard a thousand times that 'You get \u001b[0m\n",
              "\u001b[32mout what you put in.'\"\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'The month is October. The year is 2023.'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"One of the most important things that I didn't understand about the world as a child was the degree to which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the returns for performance are superlinear.\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"Teachers and coaches implicitly told us that the returns were linear. I heard a thousand times that 'You get </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">out what you put in.'\"</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fQHDbo24k96"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}